import uuid
import hashlib
import boto3
import datetime
samples = []
all_subjects = {}
fathers = {}
mothers = {}
sample_metadata = {}
seq_mapping = {}
spl_mapping = {}  # name to uuid
names = {}
uid2name = {}
seq_uuids = {}
spl_uuids = {}   # uuid to name
all_uuids = {}
pcornetids = {}
chksums = {}
buckets = {}
seq_map_file = 'sequence_mapping.txt'
seq_json = 'sequence_mapping.json'
spl_map_file = 'sample_mapping.txt'
sample_names = 'sample_names.txt'
subject_meta = 'subject_meta'
fastq_list = 'fastq_files.txt'
sample_metafile = 'sample_metadata.csv'  # from Casey's excel file
indexes = [0, 1, 2, 3, 4, 7, 9, 10, 11, 12, 29] # into sample_metadata.csv
sample_headers = []  # extracted from sample_metadata.csv
contact = "zhangs3@email.chop.edu"

# for uploading to s3 bucket
AWS_ACCESS_KEY_ID = ''
AWS_SECRET_ACCESS_KEY = ''
AWS_SECURITY_TOKEN = ''

sizelimit = config['sizelimit']   # to use multipart for uploading
readsize = config['readsize']   # size of each part to upload
bucket = config['bucket']   # s3 bucket
home = os.path.expanduser("~")

def set_key():
    global AWS_ACCESS_KEY_ID
    global AWS_SECRET_ACCESS_KEY
    global AWS_SECURITY_TOKEN
    envs = {}

    # initiates the required entries first
    envs['AWS_ACCESS_KEY_ID'] = ''
    envs['AWS_SECRET_ACCESS_KEY'] = ''
    envs['AWS_SECURITY_TOKEN'] = ''

    # the user has to make sure the keys are not expired
    fin = open(home + '/.bash_profile', "r")
    for line in fin.readlines():
        if re.search('^export AWS_', line):
             line = re.sub('^export ', '', line).strip()
             kv = line.split('=')
             envs[kv[0]] = kv[1]
             # os.environ[envs[0]] = envs[1]
             # print(envs)
             # print(os.environ)
    fin.close()
    # print(envs)

    AWS_ACCESS_KEY_ID = envs['AWS_ACCESS_KEY_ID']
    AWS_SECRET_ACCESS_KEY = envs['AWS_SECRET_ACCESS_KEY']
    AWS_SECURITY_TOKEN = envs['AWS_SECURITY_TOKEN']

def s3client():
    return boto3.client('s3',
                  aws_access_key_id=AWS_ACCESS_KEY_ID,
                  aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
                  aws_session_token=AWS_SECURITY_TOKEN
     )

def s3start(s3, file):
    return s3.create_multipart_upload(Bucket=bucket,
                                      Key=file,
                                      ServerSideEncryption='AES256',
                                      RequestPayer='requester'
                                      )

def s3upload(s3, src, tgt):
    st = os.stat(src)  # for real file if symlink, os.lstat(file) for link
    status = ''
    # print(st)
    fin = open(src, "rb")
    if st.st_size > sizelimit:
        uploadid = s3start(s3, tgt)['UploadId']
        try:
            partnum = 0
            parts = []
            data = fin.read(readsize)
            while data:
                partnum += 1
                # print(partnum)
                res = s3.upload_part(Bucket=bucket,
                                     Body=data,
                                     ContentLength=len(data),
                                     Key=tgt,
                                     UploadId=uploadid,
                                     RequestPayer='requester',
                                     PartNumber=partnum
                                     )
                # print(res)
                parts.append({'ETag':res['ETag'], 'PartNumber': partnum})
                data = fin.read(readsize)

            res = s3.complete_multipart_upload(Bucket=bucket,
                                         Key=tgt,
                                         MultipartUpload={'Parts':parts},#needed
                                         UploadId=uploadid,
                                         RequestPayer='requester')
            print(res)
        except:
            res = s3.abort_multipart_upload(Bucket=bucket,
                                                Key=tgt,
                                                UploadId=uploadid,
                                                RequestPayer='requester'
                                            )
            # print(src + ":")
            # print(str(os.sys.exc_info()[1]))
            status = str(os.sys.exc_info()[1])
    else:
        try:
            res = s3.put_object(Bucket=bucket,
                                Body=fin,
                                Key=tgt,
                                RequestPayer='requester',
                                ServerSideEncryption='AES256')

            print(res)
        except:
            # print(src + ":")
            # print(str(os.sys.exc_info()[1]))
            # quit()
            status = str(os.sys.exc_info()[1])
    fin.close()

    return status

def now():
    now = datetime.datetime.now()
    return "%s-%02d-%02d:%02d:%02d:%02d" % (now.year, now.month, now.day, now.hour, now.minute, now.second)

def get_md5(file, block_size=2**20):
# http://stackoverflow.com/questions/1131220/get-md5-hash-of-big-files-in-python
    md5 = hashlib.md5() # must get a new instance!!!!!
    with open(file,'rb') as f:
        for chunk in iter(lambda: f.read(block_size), b''):
             md5.update(chunk)
    return md5.hexdigest()

def get_mapped():
    global names
    global seq_mapping
    global seq_uuids
    global all_uuids
    global uid2name

    if os.path.exists(seq_map_file):
        fmap = open(seq_map_file, "r")
        for line in fmap.readlines():
            (uid, name, chksum, bucket) = line.strip().split()

            name = re.sub('\.fastq.gz$', '', name)
            uid = re.sub('\.fastq.gz$', '', uid)
            names[name] = uid
            uid2name[uid] = name
            chksums[name] = chksum
            buckets[uid] = bucket
            root = re.sub('_R[12].*$', '', name)
            uid = re.sub('_R[12]$', '', uid)
            seq_mapping[root] = uid
            seq_uuids[uid] = root
            all_uuids[uid] = 1
        fmap.close()

def get_fastq_files():
    fastqs = []
    
    if os.path.exists(fastq_list):
        fin = open(fastq_list, "r")
        for line in fin.readlines():
            if re.search('^\s*#', line):  # comment lines
                continue
            if re.search('^\s*$', line):  # blank lines
                continue
            name = re.sub('#.*$', '', line).strip() # also remove comments
            fastqs.append(name)
        fin.close()
    #else:
        #print(fastq_list + " doesn't exist!")

    return(fastqs)

def new_uuid():
    uid = str(uuid.uuid4())
    while uid in all_uuids:       # make sure no collision
        uid = str(uuid.uuid4())
    all_uuids[uid] = 1
    return uid

def get_uuid(name):
    global seq_mapping
    global seq_uuids
    # name = re.sub('.*/', '', name)
    # name = name.rstrip('.gz')    # make sure files are gziped
    # root = re.sub('_R[12].*\.fastq\.gz$', '', name)
    root = re.sub('_R[12].*$', '', name)
    tail = re.sub(root, '', name)
    tail = re.sub('^(...).*', r'\1', tail)

    if root in seq_mapping:
        uid = seq_mapping[root]
    else:
        uid = new_uuid()
        seq_mapping[root] = uid
        seq_uuids[uid] = root

    return uid + tail

def get_uuids():      # uuids for fastq files
    global seq_uuids
    global names
    global uid2name
    global seq_mapping

    has_new = False

    for line in get_fastq_files():
        # <bucket>:<s3_path>/<sample_file_name>
        name = re.sub('.*/', '', line)
        name = re.sub('\.fastq.gz$', '', name)
        if not name in names:
            has_new = True
            # print(name)
            bucket = re.sub('[^/]+$', '', line)  # include foler/path
            uid = get_uuid(name)   # uid includes _R1 or _R2
            buckets[uid] = bucket  # with a trailing slash (/)
            names[name] = uid
            uid2name[uid] = name
            chksums[name] = get_md5('fastq/' + name + '.fastq.gz')

            root = re.sub('_R[12].*$', '', name)
            uid = re.sub('_R[12]$', '', uid)
            seq_mapping[root] = uid
            seq_uuids[uid] = root
            all_uuids[uid] = 1
     
    if has_new:
        keys = sorted(list(names.keys()))
        with open(seq_map_file, "w") as fmap:
            for name in keys:
               # print(name)
               uid = names[name]
               fmap.write(uid + ".fastq.gz " + name + ".fastq.gz " + chksums[name] + " " + buckets[uid] + "\n")

        with open(seq_json, "w") as fmap:
            fmap.write("[\n");
            uid = ''
            for name in keys:
                # print(name)
                if uid:
                    fmap.write(",\n")
                uid = names[name]
                (bucket, path) = buckets[uid].split(':')
                path = re.sub('/$', '', path)
                fmap.write("  {\n")
                fmap.write('    "file_name": "' + uid + '.fastq.gz",' + "\n")
                fmap.write('    "original_name": "' + name + '.fastq.gz",' + "\n")
                fmap.write('    "checksum": "' + chksums[name] + '",' + "\n")
                fmap.write('    "s3_bucket": "' + bucket + '",' + "\n")
                fmap.write('    "s3_path": "' + path + '"' + "\n")
                fmap.write('  }')
            fmap.write("\n]\n")

    # return [name + '.fastq.gz' for name in names]
    # print(names.values())
    # print(uid2name)
    return [uuid + '.fastq.gz' for uuid in uid2name]

#def uuid2name(uuid):
#    return spl_uuids[uuid]
    
def get_sample_mapping():
    global spl_mapping
    global spl_uuids
    global all_uuids

    if os.path.exists(spl_map_file):
        with open(spl_map_file, "r") as fmap:
            for line in fmap.readlines():
                (uid, name) = line.strip().split()
                spl_mapping[name] = uid
                spl_uuids[uid] = name
                all_uuids[uid] = 1
            
def get_sample_names():
    global pcornetids
    global samples
    if os.path.exists(sample_names):
        with open(sample_names, "r") as fin:
            for line in fin.readlines():
                if re.search('^\s*#', line):  # comment lines
                    continue
                if re.search('^\s*$', line):  # blank lines
                    continue
                name = re.sub('\s.*', '', line)
                samples.append(name)
                pcornetid = re.sub('^[^\s]+\s*', '', re.sub('\s*#.*$', '', line.strip()))
                if len(pcornetid):
                     pcornetids[name] = pcornetid
    #else:
        # print(sample_names + " doesn't exist")
 

    # print(pcornetids)
    # return samples

def upload_mapfile():
    tgt = 'epilepsy/prospective/' + seq_map_file + '2'
    # cmd = "aws --profile risaws s3api put-object --bucket chopgrin --request-payer requester --server-side-encryption AES256 --key %s --body %s" % (tgt, seq_map_file)
    # print(cmd)
    # shell(cmd)
    set_key()
    s3 = s3client()
    status = s3upload(s3, seq_map_file, tgt)
    if status:
        print(status)
        quit()


def getname(uuid):
    return 'fastq/' + uid2name[uuid] + '.fastq.gz'

def get_meta_files():
    # uploaded = config['results']['uploaded']
    # return [config['results']['sequence_meta'] + re.sub(uploaded, '', re.sub('_R1\..*$', '.json', name)) for name in glob.glob(uploaded + "/*_R1.*.gz")]
    return [config['results']['sequence_meta'] + '/' + re.sub('_R1\..*$', '.json', name) for name in get_uuids() if re.search('_R1\.', name)]

def get_sample_meta_files():
    return [config['results']['sample_meta'] + '/' + name + '.json' for name in sample_uuids]
    #files = [config['results']['sample_meta'] + '/' + name + '.json' for name in sample_uuids]
    #print(files)
    #return files

def get_sample_uuids():
    global spl_mapping
    global spl_uuids
    global samples
    has_new = False

    for sample in samples:
        if not sample in spl_mapping:
            has_new = True
            uid = new_uuid()
            spl_mapping[sample] = uid
            spl_uuids[uid] = sample

    if has_new:
        keys = sorted(list(spl_mapping.keys()))
        with open(spl_map_file, "w") as fmap:
            for name in keys:
               # print(name)
               uid = spl_mapping[name]
               fmap.write(uid + " " + name + "\n")

    #return [uuid + '.json' for uuid in spl_uuids.keys()]
    # print(spl_uuids)
    # print(spl_mapping)
    return spl_uuids.keys()

def get_all_subjects():
    global samples
    global fathers
    global mothers
    global all_subjects

    table_file = config['sample_table']
    with open(table_file) as fin:
        for line in fin.readlines():
            vals = re.split('\s+', line.lstrip())
            if vals[1] in samples:
                if vals[2]:
                    mothers[vals[2]] = 1
                if vals[3]:
                    fathers[vals[3]] = 1
                all_subjects[vals[1]] = [vals[2], vals[3]] # either mother or father may be empty!

def get_sample_metadata():
    """
    based on Casey's excel file and may need be revised
    if data are available for other fields or the file structure's changed,
    for example, to include parents
    """
    global sample_metadata
    global sample_headers
    global indexes
    import csv
    csvfile = open(sample_metafile)
    reader = csv.DictReader(csvfile)
    # print(reader.fieldnames)

    for idx in indexes:
        sample_headers.append(re.sub('Patient', 'Subject', reader.fieldnames[idx]))
    # 0 "InstitutionalName": "CHOP",
    # 1 "PatientEthnicity": "Not Hispanic/Latino",
    # 2 "PatientRace": "White",
    # 3 "PatientSex": "M",
    # 4 "PatientBirthYear": "1956",
    # 5 "InstitutionalPatientIdentifier": "",
    # 6 "ProjectSubjectIdentifier": "EG0004F",
    # 7 "PatientPrimaryLanguage": "English",
    # 8 "InstitutionalSampleIdentifier": "",
    # 9 "ProjectName": "Epilepsy",
    # 10 "SampleCategory": "Fluid",
    # 11 "SampleType": "Whole Blood",
    # 12 "SampleSource": "Human",
    # 29 "SubjectICD10Code1": "Early onset epileptic encephalopathy,..."

    for row in reader:
        if row[reader.fieldnames[6]]:
            name = row[reader.fieldnames[6]]
            sample_metadata[name] = []
            for idx in indexes:
                key = reader.fieldnames[idx]
                sample_metadata[name].append(row[key])
    csvfile.close()

def get_sample_file(uuid):
     return 'samples/'+ spl_uuids[uuid] + '.csv'

def sample_meta_file(name):
    return config['results']['sample_meta'] + '/' + spl_mapping[name] + '.json'

# must call first
get_sample_names()
get_mapped()
get_sample_mapping()
sample_uuids = get_sample_uuids() # to have spl_mapping and spl_uuids available

#rule sample_mapping:  # not working yet
#    input: sample_names
#    output: spl_map_file
#    run:
#         get_sample_uuids()


rule upload_sample_meta:
    run:
        files = glob.glob(config['results']['sample_meta'] + "/*.json")
        path = config['buckets']['chopgrin']['prospective']['sample_meta']
        set_key()
        s3 = s3client()
        for file in files:
            tgt = path + re.sub('.*\/', '', file)
            print(file)
            print(tgt)
            status = s3upload(s3, file, tgt)
            if status:
                print(status)
                quit()

rule upload_sequence_meta:
    run:
        files = glob.glob(config['results']['sequence_meta'] + "/*.json")
        path = config['buckets']['chopgrin']['prospective']['sequence_meta']
        set_key()
        s3 = s3client()
        for file in files:
            tgt = path + re.sub('.*\/', '', file)
            print(file)
            print(tgt)
            status = s3upload(s3, file, tgt)
            if status:
                print(status)
                quit()

rule upload_fastq:
    run:
        files = glob.glob(config['datadirs']['fastq'] + "/*.fastq.gz")
        path = config['buckets']['chopgrin']['prospective']['fastq']
        set_key()
        s3 = s3client()
        for file in files:
            tgt = path + re.sub('.*\/', '', file)
            print(file)
            print(tgt)
            status = s3upload(s3, file, tgt)
            if status:
                print(status)
                quit()

rule mapping:
    # input: fastq = "fastq/{name}.fastq.gz", filelist = fastq_list
    input: fastq = lambda wildcards: getname(wildcards.name), filelist = fastq_list
    output: uuid = config['results']['mapped'] + "/{name}.fastq.gz"
    run:
        # tgt = uid2name[wildcards.name] + '.fastq.gz'
        # cmd = "ln -s ../fastq/%s mapped/%s.fastq.gz" % (tgt, wildcards.name)
        cmd = "ln -s ../%s %s" % (input.fastq, output.uuid)
        print(cmd)
        shell(cmd)

rule uploading:  # should also upload metadata
    input: config['results']['mapped'] + "/{uuid}.fastq.gz"
    output: config['results']['uploaded'] + "/{uuid}.fastq.gz"
    run:

        (bucket, path) = buckets[wildcards.uuid].split(':')
        # name = names[wildcards.uuid] + '.fastq.gz'
        tgt = path + wildcards.uuid + '.fastq.gz' # path should have a trailing slash (/)
        src = 'fastq/' + uid2name[wildcards.uuid] + '.fastq.gz'
        set_key()
        s3 = s3client()
        status = s3upload(s3, src,tgt)
        if status:
             print(status)
             quit()

        if not os.path.exist(ouput):
            cmd = "ln -s ../%s %s" % (src, output)  # so we know what to upload
            print(cmd)
            shell(cmd)

#files = get_meta_files()
#print(files)
#quit()

rule make_subject_meta:
    input: subject_meta + '.json', subject_meta + '.csv'

rule make_seq_meta:
     input: get_meta_files()

rule make_sample_meta:
     input: get_sample_meta_files()
     #output: spl_map_file
     #run:
          # the output must be created here?

rule sequence_meta:
    #input: pair1 = config['results']['uploaded'] + "/{uuid}_R1.fastq.gz", pair2 = config['results']['uploaded'] + "/{uuid}_R2.fastq.gz", sample = lambda wildcards: sample_meta_file(seq_uuids[wildcards.uuid])
    input: pair1 = config['results']['mapped'] + "/{uuid}_R1.fastq.gz", pair2 = config['results']['uploaded'] + "/{uuid}_R2.fastq.gz", sample = lambda wildcards: sample_meta_file(seq_uuids[wildcards.uuid])
    output: out = config['results']['sequence_meta'] + "/{uuid}.json"
    run:
        # print('output: ' + output.out)
        # uid = new_uuid()
        uid = spl_mapping[seq_uuids[wildcards.uuid]]
        uuid  = wildcards.uuid
        name1 = uid2name[uuid+'_R1']
        name2 = uid2name[uuid+'_R2']
        name = re.sub('_R.$', '', name1)
        (bucket, path) = buckets[uuid+'_R1'].split(':')
        chksum1 = chksums[name1]
        chksum2 = chksums[name2]
        fout = open(output.out, "w")
        fout.write('''{
  "uuid": "%s",
  "submission_type": "sequence", 
  "submission_status": "active",
  "submitter_id": "CHOP",
  "contact": "%s",
  "last_updated": "%s",
  "data_type": "fastq",
  "sample_id": "%s",
  "sample_uuid": "%s",
  "reads": [{
    "path": "http://%s.s3.amazonaws.com/%s%s_R1.fastq.gz", 
    "md5sum": "%s", 
    "internal_id": "%s.fastq",
    "flowcell": null,
    "lane": null,
    "paired_end": 1
    },{
    "path": "http://%s.s3.amazonaws.com/%s%s_R2.fastq.gz", 
    "md5sum": "%s", 
    "internal_id": "%s.fastq",
    "flowcell": null,
    "lane": null,
    "paired_end": 2
  }],
  "platform": "Illumina HiSeq 4000",
  "capture": "Nextera Rapid Capture Exome Kit",
  "lab": "The Genomics Platform",
  "sequencing_center": "The Broad Institute", 
  "technique": "WES",
  "comments": "second run"
}
''' % (uuid, contact, now(), name, uid, bucket, path, uuid, chksum1, name1, bucket, path, uuid, chksum2, name2)
)
        fout.close()

        # upload outside the pipeline for the time being.
        # path = re.sub('fastq/', '', path)
        # tgt = path + output.out
        # cmd = "aws --profile saml s3api put-object --bucket %s --request-payer requester --server-side-encryption AES256 --key %s --body %s" % (bucket, tgt, output.out)
        # print(cmd)
        # shell(cmd)

rule sample_meta:
    #input: sample = lambda wildcards: uuid2name(wildcards.uuid)
    # input: spl_map_file
    input: sample_names   # supplied
    output: config['results']['sample_meta'] + "/{uuid}.json"
    run:
        fout = open(config['results']['sample_meta'] + '/' + wildcards.uuid + '.json', "w")
        if spl_uuids[wildcards.uuid] in pcornetids:
             pcornetid = '"' + pcornetids[spl_uuids[wildcards.uuid]] + '"'
        else:
             pcornetid = 'null'
             
        fout.write('''{
  "submitter_id": "CHOP",
  "sample": "%s",
  "pcornet_id": %s
}''' % (spl_uuids[wildcards.uuid], pcornetid))
        fout.close()
#  "sample_type_id": null, 
#  "time_between_excision_and_freezing": null, 
#  "oct_embedded": null, 
#  "tumor_code_id": null, 
#  "submitter_id": null, 
#  "intermediate_dimension": null, 
#  "is_ffpe": null, 
#  "pathology_report_uuid": null, 
#  "tumor_descriptor": null, 
#  "sample_type": null, 
#  "project_id": null, 
#  "current_weight": null, 
#  "composition": null, 
#  "time_between_clamping_and_freezing": null, 
#  "shortest_dimension": null, 
#  "tumor_code": null, 
#  "tissue_type": null, 
#  "days_to_sample_procurement": null, 
#  "cases": {
#    "submitter_id": null
#  }, 
#  "freezing_method": null, 
#  "type": "sample", 
#  "preservation_method": null, 
#  "days_to_collection": null, 
#  "initial_weight": null, 
#  "longest_dimension": null

rule subject_meta:
    input: spl_map_file, sample_metafile
    output: json=subject_meta + '.json', csv=subject_meta + '.csv'
    run:
        # print(spl_mapping)
        # print("=========")
        # print(spl_uuids)
        # print(pcornetids)

        get_sample_metadata()

        get_all_subjects()
        # print(all_subjects)

        date = now()
 
        fjson = open(output.json, "w")
        fcsv = open(output.csv, "w")

        # may need an explicit list of probands to import
        subjects = sorted(list(all_subjects.keys()))
        headers = ['SubjectUUID','FatherUUID','MotherUUID','SubjectId','FatherId','MotherId','PcornetId']
        rng1 = range(1, len(headers))
        rng2 = range(len(sample_headers))

        fcsv.write(headers[0])
        for idx in rng1:
            fcsv.write(',' + headers[idx])
        fcsv.write(',SubjectType')
        for idx in rng2:
            fcsv.write(',' + sample_headers[idx])
        fcsv.write('Contact,LastUpdated')
        fcsv.write("\n")
        fjson.write("[\n")
        uuid = ''
        for subject in subjects:
            if uuid:
                fjson.write(",\n")
            uuid = spl_mapping[subject]
            mother = all_subjects[subject][0]
            father = all_subjects[subject][1]
            if mother:
                muuid = spl_mapping[mother]
                mspl_id = spl_uuids[muuid]
            else:
                muuid = ''
                mspl_id = ''
            if father:
                fuuid = spl_mapping[father]
                fspl_id = spl_uuids[fuuid]
            else:
                fuuid = ''
                fspl_id = ''
            vals = [uuid,fuuid,muuid,subject,fspl_id,mspl_id,pcornetids[subject]]
            fcsv.write(uuid)
            fjson.write("  {\n")
            fjson.write('    "' + headers[0] + '": "' + vals[0] +'",' + "\n")
            for idx in rng1:
                fcsv.write(',' + vals[idx])
                fjson.write('    "' + headers[idx] + '": "' + vals[idx] +'",' + "\n")

            # attempt to handle both proband and parents, may need be revised
            if mother and father:  # have both mother and father. should consider cases of single parent?
                fcsv.write(',proband') 
                fjson.write('    "SubjectType": "proband",' + "\n")
            elif subject in mothers:
                fcsv.write(',mother') 
                fjson.write('    "SubjectType": "mother",' + "\n")  # right now no consideration of childs
            elif subject in fathers:
                fcsv.write(',father') 
                fjson.write('    "SubjectType": "father",' + "\n")  # right now no consideration of childs
            else:
                fcsv.write(',unknown') 
                fjson.write('    "SubjectType": "unknown",' + "\n")

            for idx in rng2:
                if re.search(',', sample_metadata[subject][idx]):
                    fcsv.write(',"' + sample_metadata[subject][idx]+'"')
                else:
                    fcsv.write(',' + sample_metadata[subject][idx])
                fjson.write('    "' + sample_headers[idx] + '": "' + sample_metadata[subject][idx] + '",' + "\n")

            fcsv.write(',' + contact)
            fcsv.write(',' + date)
            fcsv.write("\n")
            fjson.write('    "Contact": "' + contact + '",' + "\n")
            fjson.write('    "LastUpdated": "' + date + '"' + "\n")
            fjson.write("  }")
        fjson.write("\n]\n")
        fcsv.close()
        fjson.close()
